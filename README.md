# âœˆï¸ RL + XFOIL Airfoil Optimizer

Optimize NACA-like airfoils using **Reinforcement Learning + Aerodynamic Modeling**.  
A PPO agent learns to tune airfoil parameters \[m, p, t\] (max camber, camber position, thickness) to improve **lift-to-drag ratio (L/D)** across multiple angles of attack.

---

## ğŸŒ Live Demo

ğŸ‘‰ **Streamlit App:** _add your deployed URL here, e.g._  
`https://airfoil-rl-optimizer.streamlit.app`

---

## ğŸ“¸ Screenshots

> Replace the image paths with your actual files (for example, put PNGs in `assets/`).

### 1. Main UI â€“ RL + XFOIL Airfoil Optimizer

![Main App Screenshot](assets/app_main.png)

### 2. Baseline vs RL-Optimized Airfoil

![Optimized Airfoil Screenshot](assets/optimized_airfoil.png)

---

## ğŸš€ Key Features

- ğŸ§  **PPO Reinforcement Learning** using Stable-Baselines3.
- ğŸŒ€ **Custom Gymnasium Environment** for NACA-like airfoil optimization.
- ğŸ“ **NACA 4-Digit Airfoil Generator** (`airfoil_gen.py`).
- ğŸ“Š **Multi-Angle Evaluation** at 0Â°, 4Â°, 8Â° using a smooth surrogate aero model.
- ğŸ¯ **Reward = Mean L/D + Lift Bonus âˆ’ Deviation Penalty** for realistic designs.
- ğŸ’» **Streamlit Web App (`app.py`)** for interactive exploration and optimization.
- ğŸ“ˆ **Baseline vs RL-Optimized Comparison** with metrics and plots.

---

## ğŸ§© Airfoil Parameters

The airfoil is described by classic NACA-style parameters:

| Symbol | Description     | Typical Range |
| ------ | --------------- | ------------- |
| `m`    | Max camber      | 0.00 â€“ 0.06   |
| `p`    | Camber position | 0.10 â€“ 0.70   |
| `t`    | Thickness       | 0.11 â€“ 0.18   |

The RL agent learns small updates Î”\[m, p, t\] within these bounds.

---

## ğŸ“‚ Project Structure

````text
airfoil-rl-optimizer/
â”‚
â”œâ”€â”€ models/                     # Trained PPO agent(s)
â”‚   â””â”€â”€ ppo_airfoil_fake.zip
â”‚
â”œâ”€â”€ assets/                     # Screenshots / images for README & app
â”‚   â”œâ”€â”€ app_main.png
â”‚   â””â”€â”€ optimized_airfoil.png
â”‚
â”œâ”€â”€ aero_eval.py                # Surrogate aerodynamic model (Cl, Cd, L/D)
â”œâ”€â”€ airfoil_env.py              # Custom Gymnasium environment
â”œâ”€â”€ airfoil_gen.py              # NACA 4-digit airfoil geometry generator
â”œâ”€â”€ analyze_policy.py           # Evaluate and plot best RL airfoil
â”œâ”€â”€ compare_multi.py            # Baseline vs optimized L/D across AoA
â”œâ”€â”€ test_env.py                 # Simple environment sanity checks
â”œâ”€â”€ train_rl.py                 # PPO training script
â”‚
â”œâ”€â”€ app.py                      # Streamlit web app entry point
â”‚
â”œâ”€â”€ requirements.txt            # Python dependencies
â”œâ”€â”€ .gitignore
â”œâ”€â”€ LICENSE                     # MIT License
â””â”€â”€ README.md

---

## âš™ï¸ Installation & Setup

### 1ï¸âƒ£ Clone the Repository

```bash
git clone https://github.com/mohamednoorulnaseem/airfoil-rl-optimizer.git
cd airfoil-rl-optimizer
````

### 2ï¸âƒ£ Create and Activate Environment (Conda Recommended)

```bash
conda create -n airfoil_rl python=3.10 -y
conda activate airfoil_rl
```

### 3ï¸âƒ£ Install Required Packages

```bash
pip install -r requirements.txt
```

---

## ğŸ§  Train the PPO Agent (Optional)

A pretrained model is included, but you can train again:

```bash
python train_rl.py
```

This will:

- Train a PPO policy on the custom Airfoil environment
- Save the model in the `models/` directory

---

## ğŸ“Š Compare Multi-Angle Performance (0Â°, 4Â°, 8Â°)

```bash
python compare_multi.py
```

Example Output (will vary):

```
AoA |  L/D baseline  |  L/D optimized
----------------------------------------
 0.0 |         8.18 |         10.00
 4.0 |        30.00 |         20.91
 8.0 |        19.09 |         17.06
```

---

## ğŸŒ Run the Streamlit App Locally

```bash
streamlit run app.py
```

Features include:

- Manual slider for airfoil parameters
- RL-Optimized parameters using PPO agent
- Airfoil geometry plots
- L/D metrics table
- Multi-AoA comparison

---

## â˜ï¸ Deploying to Streamlit Cloud

1. Push this repository to GitHub
2. Go to: https://share.streamlit.io/
3. Provide:
   - **Repo:** `mohamednoorulnaseem/airfoil-rl-optimizer`
   - **Main file:** `app.py`
   - **Python version:** `3.10`

> No secret keys are required for this project.

---

## ğŸ§® Reward & RL Design Overview

The RL agent receives the state:

```
[m, p, t, Cl_mid, Cd_mid]
```

Actions are small continuous changes to parameters:

```
Î”[m, p, t] âˆˆ [-0.005, 0.005] Ã— [-0.05, 0.05] Ã— [-0.01, 0.01]
```

The reward encourages realistic and efficient airfoils:

```
reward = mean(L/D at 0Â°,4Â°,8Â°)
         + 0.5 * Cl_mid
         - 0.05 * distance_from_baselineÂ²
```

---

## ğŸ§¾ License

This project is licensed under the **MIT License**.  
You may modify, distribute, and use it with attribution.

---

## ğŸ‘¨â€ğŸ’» Author

**Mohamed Noorul Naseem**  
GitHub: https://github.com/mohamednoorulnaseem  
If you like this project, donâ€™t forget to â­ the repo!

---

âœˆï¸ _Happy Airfoil Optimization!_ ğŸ§ 
